{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction, Loading & Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Load and Concatenate data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_files(file_path, chunk_size=10**4, separator='$'):\n",
    "    \"\"\"\n",
    "    Loads and concatenates data from a list of file paths with error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): File path to read.\n",
    "    chunk_size (int): Number of rows per chunk to read at a time.\n",
    "    separator (str): Delimiter for the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated DataFrame from all the file chunks if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='ASCII', errors='replace') as file:\n",
    "            for chunk in pd.read_csv(file, sep=separator, chunksize=chunk_size, on_bad_lines='skip'):\n",
    "                chunks.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if chunks:\n",
    "        try:\n",
    "            concatenated_df = pd.concat(chunks, ignore_index=True)\n",
    "            concatenated_df.columns = concatenated_df.columns.str.lower()\n",
    "            return concatenated_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating data: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No data to concatenate.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the year by quarter FAERS data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: ../final dataset/Drugs/2024/Ventolin_df_2024Q1.csv\n",
      "File saved: ../final dataset/Drugs/2024/Ventolin_df_2024Q2.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For loop:\n",
    "This code script iterates through the FAERS data files from years 2013 to 2023\n",
    "\n",
    "Load and concatenate:\n",
    "It applies the load_and_concatenate function, and merges all the data files into a final_df.\n",
    "\n",
    "Create and save the final dataframe:\n",
    "This is the final data extraction step before any data cleaning or anaylsis is performed.\n",
    "'''\n",
    "\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "all_data = []\n",
    "\n",
    "# Loop through the years 2013 to 2023\n",
    "for year in range(2013, 2023):\n",
    "    # Loop through the quarters Q1 to Q4\n",
    "    for quarter in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        \n",
    "        # Construct the file names\n",
    "        drugs_file = f'../drugs files/faers_ascii_{year}{quarter}/ASCII/DRUG{str(year)[-2:]}{quarter}.txt'\n",
    "        indications_file = f'../drugs files/faers_ascii_{year}{quarter}/ASCII/INDI{str(year)[-2:]}{quarter}.txt'\n",
    "        reactions_file = f'../drugs files/faers_ascii_{year}{quarter}/ASCII/REAC{str(year)[-2:]}{quarter}.txt'\n",
    "        reportsource_file = f'../drugs files/faers_ascii_{year}{quarter}/ASCII/RPSR{str(year)[-2:]}{quarter}.txt'\n",
    "        demo_file = f'../drugs files/faers_ascii_{year}{quarter}/ASCII/DEMO{str(year)[-2:]}{quarter}.txt'\n",
    "\n",
    "\n",
    "        # Load and concatenate all the FAERS data files\n",
    "        reactions = load_and_concatenate_files(reactions_file)\n",
    "        reactions.drop(columns=['caseid'], inplace=True)\n",
    "\n",
    "        indications = load_and_concatenate_files(indications_file)\n",
    "        indications.drop(columns=['caseid'], inplace=True)\n",
    "\n",
    "        drugs = load_and_concatenate_files(drugs_file)\n",
    "        drugs['drugname'] = drugs['drugname'].fillna('') # Fill NaN values in 'drugname' with an empty string\n",
    "        drugs = drugs[drugs['drugname'].str.contains('VENTOLIN')] # Filter by Ventolin drugname\n",
    "        drugs.drop(columns=['caseid'], inplace=True) # drop the caseid column\n",
    "\n",
    "        reportsource = load_and_concatenate_files(reportsource_file)\n",
    "        reportsource.drop(columns=['caseid'], inplace=True) # drop the caseid column\n",
    "\n",
    "        demographics = load_and_concatenate_files(demo_file)\n",
    "        demographics.drop(columns=['caseid'], inplace=True) # drop the caseid column\n",
    "\n",
    "        # Merged files\n",
    "        drugs_react = pd.merge(drugs,reactions, on = 'primaryid', how = 'inner')\n",
    "        drugs_reac_indi = pd.merge(drugs_react,indications, on = 'primaryid', how = 'inner')\n",
    "        drugs_reac_indi_demo = pd.merge(drugs_reac_indi,demographics, on = 'primaryid', how = 'inner')\n",
    "        drugs_reac_indi_demo_source = pd.merge(drugs_reac_indi_demo,reportsource, on = 'primaryid', how = 'inner')\n",
    "        \n",
    "        # Final_df is comprised of the merged dataframes of the drugs, reactions, indications, demogrphics, and report source code.\n",
    "        final_df = drugs_reac_indi_demo_source.copy()\n",
    "\n",
    "        try:\n",
    "            # Create the directory if it doesn't exist\n",
    "            output_dir = f'../final dataset/Drugs/{year}'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Save the CSV file - Filtered by Ventolin\n",
    "            output_file = os.path.join(output_dir, f'Ventolin_df_{year}{quarter}.csv')\n",
    "            \n",
    "            # Save the CSV file - Unfiltered by any drug (requried if analysis of non ventolin drugs vs ventolin drugs is pursued)\n",
    "            # output_file = os.path.join(output_dir, f'Drugs_df_{year}{quarter}.csv')\n",
    "            \n",
    "            # final df saved to output directory with output file name\n",
    "            final_df.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"File saved: {output_file}\")\n",
    "    \n",
    "\n",
    "        except FileNotFoundError:\n",
    "                    print(f\"File not found\") # catch any errors if output has failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ventolin data - Construct final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ../final dataset/Drugs/2024/Ventolin_df_2024Q3.csv\n",
      "File not found: ../final dataset/Drugs/2024/Ventolin_df_2024Q4.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store dataframes\n",
    "all_data = []\n",
    "\n",
    "# Loop through the years 2013 to 2023\n",
    "for year in range(2013, 2023):\n",
    "    # Loop through the quarters Q1 to Q4\n",
    "    for quarter in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        # Construct the file name\n",
    "        file_name = f\"../final dataset/Drugs/{year}/Ventolin_df_{year}{quarter}.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_name)\n",
    "            # Optionally, add a column to track the year and quarter\n",
    "            df['year_quarter'] = f'{year}_{quarter}'\n",
    "            df['year'] = year\n",
    "            df['quarter'] = quarter\n",
    "            # Append the dataframe to the list\n",
    "            all_data.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "\n",
    "# Combine all dataframes into a single dataframe\n",
    "final_df = pd.concat(all_data, ignore_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframe\n",
    "final_df.to_csv('../final dataset/Final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
